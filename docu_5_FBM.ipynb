{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1035a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffd5a7",
   "metadata": {},
   "source": [
    "FBM_LSTMä¸å¦‚LSTMå¥½ç”¨ï¼ŒLSTM/GRU æ˜¯å±€éƒ¨å»ºæ¨¡å™¨ï¼Œåªé€‚åˆçŸ­æœŸè®°å¿†ï¼Œæˆ‘æ‰“ç®—ç”¨FBM-NPï¼Œæ›´é€‚åˆé•¿æœŸé¢„æµ‹\n",
    "\n",
    "æˆ‘æ‰“ç®—ç”¨FBM-NL + å¤šæ­¥æ”¶ç›Šç‡é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de06eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#é…ç½®å‚æ•°\n",
    "T = 128  # æ¯ä¸ªæ»‘åŠ¨çª—å£çš„æ—¶é—´åºåˆ—é•¿åº¦ï¼ˆå³æ¨¡å‹è¾“å…¥åºåˆ—é•¿åº¦ï¼‰\n",
    "L = 1    # é¢„æµ‹æ­¥æ•°ï¼ˆä¾‹å¦‚é¢„æµ‹ä¸‹ä¸€ä¸ª close ä»·æ ¼ï¼‰\n",
    "stride = 1 #æ»‘åŠ¨çª—å£çš„æ­¥é•¿\n",
    "num_epochs = 50 # æ¨¡å‹è®­ç»ƒçš„è½®æ•°\n",
    "batch_size = 32 # æ¯ä¸€æ‰¹è®­ç»ƒæ ·æœ¬çš„æ•°é‡\n",
    "\n",
    "# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼ˆå¦‚æœæœ‰GPUå°±ç”¨GPUï¼Œå¦åˆ™ç”¨CPUï¼‰\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c3ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®è¯»å–ä¸æ»‘çª—\n",
    "df = pd.read_csv('SP500.csv', parse_dates=['date'])\n",
    "df.sort_values('date', inplace=True)\n",
    "close = df['close'].values\n",
    "returns = np.diff(np.log(close))  # å¯¹æ•°çš„æ”¶ç›Šç‡\n",
    "\n",
    "# æ£€æŸ¥è¾“å…¥é•¿åº¦Tæ˜¯å¦æ˜¯å¶æ•°\n",
    "if T % 2 != 0:\n",
    "    T += 1\n",
    "\n",
    "# ç”Ÿæˆæ»‘çª—å’Œç›®æ ‡\n",
    "data = np.lib.stride_tricks.sliding_window_view(returns, window_shape=T)[::stride]\n",
    "y = returns[T + (L - 1):]  # é¢„æµ‹Læ­¥åçš„å€¼\n",
    "X_raw = data[:len(y)]  # é€‚é…Xå’Œyçš„é•¿åº¦\n",
    "X_raw = (X_raw - X_raw.mean()) / (X_raw.std() + 1e-8)  # åŸå§‹è¾“å…¥å¹³ç¨³åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5970204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€šç”¨ Dataset ç±»\n",
    "#å°†è¾“å…¥ç‰¹å¾ X å’Œç›®æ ‡æ ‡ç­¾ y å°è£…ä¸º PyTorch å¯å¤„ç†çš„æ•°æ®é›†ï¼Œå¥½è¿›è¡Œä¸€äº›æ“ä½œ\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)#å°†è¾“å…¥ç‰¹å¾è½¬æ¢ä¸º float32 ç±»å‹çš„å¼ é‡\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)#åœ¨æœ€åä¸€ç»´æ·»åŠ ç»´åº¦\n",
    "    def __len__(self):#è¿”å›æ•°æ®é›†çš„æ ·æœ¬æ•°é‡\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):#æ ¹æ®ç´¢å¼• idx è¿”å›å¯¹åº”çš„ç‰¹å¾å’Œæ ‡ç­¾æ ·æœ¬\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBMç‰¹å¾ç”Ÿæˆå‡½æ•°\n",
    "#è¾“å…¥é•¿åº¦ä¸ºTçš„ä¸€ç»´æ•°ç»„ï¼Œè¾“å‡ºäºŒç»´æ•°ç»„\n",
    "def fbm_features(x):\n",
    "    T = len(x)\n",
    "    H = np.fft.fft(x) # å‚…é‡Œå¶å˜æ¢\n",
    "    HR, HI = H.real, H.imag\n",
    "    K = T // 2 + 1\n",
    "    N = np.arange(T).reshape(-1, 1)# æ—¶é—´ç´¢å¼•åˆ—å‘é‡ (T, 1)\n",
    "    ks = np.arange(K).reshape(1, -1)# é¢‘ç‡ç´¢å¼•è¡Œå‘é‡ (1, K)\n",
    "\n",
    "    # ç”Ÿæˆæ­£äº¤åŸºå‡½æ•°çŸ©é˜µ (æ–‡çŒ®å…¬å¼5å’Œ6)\n",
    "    C = np.cos(2 * np.pi * N @ ks / T)/T\n",
    "    S = -np.sin(2 * np.pi * N @ ks / T)/T\n",
    "\n",
    "    #åˆå§‹åŒ–é¢‘åŸŸç³»æ•° (æ–‡çŒ®å…¬å¼7å’Œ8)\n",
    "    ak = np.zeros(K)\n",
    "    bk = np.zeros(K)\n",
    "    ak[0] = HR[0]          # ç›´æµåˆ†é‡\n",
    "    bk[0] = 0              # ç›´æµåˆ†é‡æ— è™šéƒ¨\n",
    "    ak[-1] = HR[T//2]      # å¥ˆå¥æ–¯ç‰¹é¢‘ç‡åˆ†é‡\n",
    "    bk[-1] = 0             # å¥ˆå¥æ–¯ç‰¹é¢‘ç‡åˆ†é‡æ— è™šéƒ¨\n",
    "    ak[1:-1] = 2 * HR[1:T//2]  # ä¸­é—´é¢‘ç‡åˆ†é‡å®éƒ¨ç¼©æ”¾\n",
    "    bk[1:-1] = 2 * HI[1:T//2]  # ä¸­é—´é¢‘ç‡åˆ†é‡è™šéƒ¨ç¼©æ”¾\n",
    "    \n",
    "    fbm_feature = np.concatenate([C * ak, S * bk], axis=1)\n",
    "    fbm_feature = (fbm_feature - fbm_feature.mean()) / (fbm_feature.std() + 1e-8)\n",
    "    return fbm_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de926345",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbm_tensor = np.stack([fbm_features(x) for x in X_raw], axis=0)\n",
    "X_fbm = fbm_tensor[:len(y)]\n",
    "X_raw = X_raw[:, :, np.newaxis]  # è¾“å…¥è°ƒæ•´ç»´åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de5521c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¡ºåºåˆ†å‰²æ•°æ®ï¼Œé˜²æ­¢æ—¶é—´æ··æ·†\n",
    "train_size = int(0.8 * len(X_raw))\n",
    "train_raw = SeqDataset(X_raw[:train_size], y[:train_size])\n",
    "val_raw = SeqDataset(X_raw[train_size:], y[train_size:])\n",
    "train_fbm = SeqDataset(X_fbm[:train_size], y[:train_size])\n",
    "val_fbm = SeqDataset(X_fbm[train_size:], y[train_size:])\n",
    "\n",
    "train_loader_raw = DataLoader(train_raw, batch_size=batch_size, shuffle=True)\n",
    "val_loader_raw = DataLoader(val_raw, batch_size=batch_size)\n",
    "train_loader_fbm = DataLoader(train_fbm, batch_size=batch_size, shuffle=True)\n",
    "val_loader_fbm = DataLoader(val_fbm, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08c7b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM ç½‘ç»œç»“æ„\n",
    "#LSTMModel æ˜¯ä¸€ä¸ªç»§æ‰¿è‡ª PyTorch nn.Module çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼šLSTMå±‚ï¼ˆæå–åºåˆ—ä¸­çš„æ—¶åºç‰¹å¾ï¼‰+å…¨è¿æ¥å±‚ï¼ˆå°† LSTM çš„è¾“å‡ºæ˜ å°„åˆ°æœ€ç»ˆé¢„æµ‹å€¼ï¼‰\n",
    "class FBM_LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.norm(out[:, -1, :])\n",
    "        return self.mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6a20b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯å‡½æ•°\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            pred = model(x_batch).cpu().numpy()\n",
    "            y_true.append(y_batch.numpy())\n",
    "            y_pred.append(pred)\n",
    "    y_true = np.concatenate(y_true).flatten()\n",
    "    y_pred = np.concatenate(y_pred).flatten()\n",
    "    return mean_absolute_error(y_true, y_pred), np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a323b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå‡½æ•°\n",
    "def train_model(model, train_loader, val_loader,patience=5):\n",
    "    model.to(device)#å°†æ¨¡å‹ç§»è‡³æŒ‡å®šè®¾å¤‡ï¼ˆCPU/GPUï¼‰\n",
    "    #Adam ä¼˜åŒ–å™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience//2)\n",
    "    criterion = nn.MSELoss()#å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°\n",
    "    best_val_mae = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()#è®¾ç½®è®­ç»ƒæ¨¡å¼ï¼šå¯ç”¨ Dropout ç­‰è®­ç»ƒä¸“ç”¨å±‚\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()#é˜²æ­¢æ¢¯åº¦ç´¯ç§¯\n",
    "            loss = criterion(model(x_batch), y_batch)\n",
    "            loss.backward()#åå‘ä¼ æ’­,è®¡ç®—æ¢¯åº¦\n",
    "            optimizer.step()#æ›´æ–°æ¨¡å‹æƒé‡\n",
    "            total_loss += loss.item()#è®°å½•æœ¬è½®æ€»æŸå¤±\n",
    "        \n",
    "\n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        val_mae, val_rmse = evaluate_model(model, val_loader)\n",
    "        scheduler.step(val_mae)\n",
    "\n",
    "        \n",
    "        # æ—©åœæœºåˆ¶\n",
    "        if val_mae < best_val_mae:\n",
    "            best_val_mae = val_mae\n",
    "            best_model = model.state_dict()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d}: TrainLoss={total_loss/len(train_loader):.4f}, Val MAE={val_mae:.4f}, RMSE={val_rmse:.4f}\")\n",
    "\n",
    "    \n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, best_val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08c6af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Training baseline LSTM...\n",
      "Epoch 01: TrainLoss=0.0126, Val MAE=0.0066, RMSE=0.0122\n",
      "Epoch 02: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0123\n",
      "Epoch 03: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 04: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 05: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 06: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 07: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 08: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0120\n",
      "Epoch 09: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0120\n",
      "Epoch 10: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0120\n",
      "Epoch 11: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 12: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Epoch 13: TrainLoss=0.0002, Val MAE=0.0066, RMSE=0.0121\n",
      "Early stopping at epoch 14\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ baseline LSTMï¼ˆåŸå§‹åºåˆ—ï¼‰:å®šä¹‰æ¨¡å‹ã€è®­ç»ƒæ¨¡å‹ã€è¯„ä¼°æ¨¡å‹\n",
    "print(\"\\nğŸ”¹ Training baseline LSTM...\")\n",
    "model_raw = FBM_LSTM(input_dim=1)  # ä¿æŒåŒä¸€ç»“æ„\n",
    "model_raw, _ = train_model(model_raw, train_loader_raw, val_loader_raw)\n",
    "mae_raw, rmse_raw = evaluate_model(model_raw, val_loader_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6777778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Training FBM-LSTM...\n",
      "Epoch 01: TrainLoss=0.0088, Val MAE=0.0091, RMSE=0.0136\n",
      "Epoch 02: TrainLoss=0.0003, Val MAE=0.0102, RMSE=0.0142\n",
      "Epoch 03: TrainLoss=0.0003, Val MAE=0.0105, RMSE=0.0144\n",
      "Epoch 04: TrainLoss=0.0002, Val MAE=0.0080, RMSE=0.0131\n",
      "Epoch 05: TrainLoss=0.0002, Val MAE=0.0070, RMSE=0.0123\n",
      "Epoch 06: TrainLoss=0.0002, Val MAE=0.0071, RMSE=0.0123\n",
      "Epoch 07: TrainLoss=0.0002, Val MAE=0.0084, RMSE=0.0130\n",
      "Epoch 08: TrainLoss=0.0002, Val MAE=0.0068, RMSE=0.0122\n",
      "Epoch 09: TrainLoss=0.0002, Val MAE=0.0082, RMSE=0.0128\n",
      "Epoch 10: TrainLoss=0.0002, Val MAE=0.0067, RMSE=0.0122\n",
      "Epoch 11: TrainLoss=0.0002, Val MAE=0.0069, RMSE=0.0123\n",
      "Epoch 12: TrainLoss=0.0002, Val MAE=0.0067, RMSE=0.0120\n",
      "Epoch 13: TrainLoss=0.0001, Val MAE=0.0070, RMSE=0.0128\n",
      "Epoch 14: TrainLoss=0.0001, Val MAE=0.0067, RMSE=0.0120\n",
      "Epoch 15: TrainLoss=0.0001, Val MAE=0.0068, RMSE=0.0121\n",
      "Epoch 16: TrainLoss=0.0001, Val MAE=0.0069, RMSE=0.0123\n",
      "Early stopping at epoch 17\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒ FBM-LSTMï¼ˆFBMç‰¹å¾ï¼‰\n",
    "print(\"\\nğŸ”¹ Training FBM-LSTM...\")\n",
    "model_fbm = FBM_LSTM(input_dim=X_fbm.shape[2])\n",
    "model_fbm, _ = train_model(model_fbm, train_loader_fbm, val_loader_fbm)\n",
    "mae_fbm, rmse_fbm = evaluate_model(model_fbm, val_loader_fbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final Evaluation:\n",
      "LSTM       â†’ MAE: 0.0066, RMSE: 0.0121\n",
      "FBM-LSTM   â†’ MAE: 0.0068, RMSE: 0.0123\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºæœ€ç»ˆå¯¹æ¯”ç»“æœ\n",
    "print(\"\\nâœ… Final Evaluation:\")\n",
    "print(f\"LSTM       â†’ MAE: {mae_raw:.4f}, RMSE: {rmse_raw:.4f}\")\n",
    "print(f\"FBM-LSTM   â†’ MAE: {mae_fbm:.4f}, RMSE: {rmse_fbm:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
